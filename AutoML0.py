"""
AutoML with LightGBM â€“ æœªæ¥äºˆæ¸¬ãƒ¢ãƒ¼ãƒ‰ä»˜ãï¼ˆã‚³ãƒ¡ãƒ³ãƒˆ & Docstring å®Œå…¨ç‰ˆï¼‰
========================================================================

æ¦‚è¦
----
ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸ CSV ã‚’ç”¨ã„ã€LightGBM ã«ã‚ˆã‚‹è‡ªå‹•ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’è¡Œã†
Streamlit ã‚¢ãƒ—ãƒªã§ã™ã€‚ä¸‹è¨˜ã®æ©Ÿèƒ½ã‚’å‚™ãˆã¦ã„ã¾ã™ã€‚

1. **é€šå¸¸ãƒ¢ãƒ¼ãƒ‰** : å›å¸°ã¾ãŸã¯åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•æ§‹ç¯‰ã—è©•ä¾¡
2. **æ™‚ç³»åˆ—ãƒ¢ãƒ¼ãƒ‰** : è¡Œç•ªå·ã§ *n* è¡Œå…ˆã®ç›®çš„å¤‰æ•°ã‚’äºˆæ¸¬ï¼ˆæœªæ¥äºˆæ¸¬ï¼‰
3. **RandomizedSearchCV** : æœ€å¤§ 30 è©¦è¡Œã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©æ¢ç´¢
4. **SHAP** : ãƒœã‚¿ãƒ³æŠ¼ä¸‹ã§ãƒ¢ãƒ‡ãƒ«è§£é‡ˆã‚’å®Ÿè¡Œ
5. **AI è§£èª¬** : SHAP çµæœã‚’å«ã‚€ãƒ“ã‚¸ãƒã‚¹å‘ã‘ãƒ¬ãƒãƒ¼ãƒˆã‚’
   OpenAI API ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
6. **ãƒ¢ãƒ‡ãƒ« / ãƒã‚¤ãƒ‘ãƒ© ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰** : pkl / json å½¢å¼ã§ä¿å­˜å¯èƒ½
"""

from __future__ import annotations

import io
import json
import pickle
from datetime import datetime
from typing import Dict, Optional, Tuple

import lightgbm as lgb
import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st
from scipy.stats import randint, uniform
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    mean_squared_error,
    r2_score,
)
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.preprocessing import LabelEncoder

# -- OpenAI åˆæœŸåŒ– ----------------------------------------------------------
try:
    from openai import OpenAI  # SDK â‰¥ 1.0

    _OPENAI_OK = True
except ImportError:  # OpenAI SDK ãŒç„¡ã„å ´åˆã§ã‚‚ã‚¢ãƒ—ãƒªã¯å‹•ä½œå¯èƒ½
    _OPENAI_OK = False


# ==========================================================================
#                               UTILITIES
# ==========================================================================
def detect_encoding(b: bytes) -> Optional[str]:
    """
    ãƒã‚¤ãƒˆåˆ—ã®å…ˆé ­ 10 KB ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¨å®šã™ã‚‹ã€‚

    1. `charset_normalizer` ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚Œã°ãã¡ã‚‰ã‚’å„ªå…ˆ  
    2. ç„¡ã‘ã‚Œã° `chardet` ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦åˆ©ç”¨

    Parameters
    ----------
    b : bytes
        èª­ã¿è¾¼ã‚“ã ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒˆåˆ—

    Returns
    -------
    str or None
        æ¨å®šã—ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åã€‚åˆ¤å®šä¸èƒ½ã®å ´åˆã¯ `None`
    """
    try:
        # Python 3.11 ä»¥é™ã§ã¯æ¨™æº–æ·»ä»˜
        from charset_normalizer import from_bytes

        best = from_bytes(b[:10_000]).best()
        return best.encoding if best else None
    except ImportError:
        import chardet

        res = chardet.detect(b[:10_000])
        return res["encoding"] or None


def load_data(uploaded_file, enc_manual: Optional[str] = None) -> Optional[pd.DataFrame]:
    """
    Streamlit FileUploader ã§å—ã‘å–ã£ãŸ CSV ã‚’èª­ã¿è¾¼ã¿ã€
    æœ€å¤§ 5,000 è¡Œã«åˆ¶é™ã—ã¦è¿”ã™ã€‚

    Parameters
    ----------
    uploaded_file : UploadedFile
        `st.file_uploader` ãŒè¿”ã™ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    enc_manual : str, optional
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ç¤ºçš„ã«é¸æŠã—ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

    Returns
    -------
    pd.DataFrame or None
        èª­ã¿è¾¼ã¿æˆåŠŸæ™‚ã¯ DataFrameã€å¤±æ•—ã—ãŸå ´åˆã¯ None
    """
    if uploaded_file is None:
        return None

    try:
        b = uploaded_file.getvalue()
        enc = enc_manual or detect_encoding(b) or "utf-8"
        # nrows ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æŠ‘åˆ¶
        df = pd.read_csv(io.BytesIO(b), encoding=enc, nrows=5_000)
        if len(df) == 5_000:
            st.info("5,000 è¡Œã‚’è¶…ãˆãŸãŸã‚å…ˆé ­ 5,000 è¡Œã®ã¿èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        return df
    except Exception as e:  # ä¾‹å¤–ã®è©³ç´°ã¯ãƒ¦ãƒ¼ã‚¶ã«è¡¨ç¤º
        st.error(f"CSV èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None


def determine_task(df: pd.DataFrame, target: str) -> str:
    """
    ç›®çš„å¤‰æ•°ã®ãƒ‡ãƒ¼ã‚¿å‹ã‹ã‚‰å›å¸° or åˆ†é¡ã‚’ã–ã£ãã‚Šåˆ¤å®šã€‚

    Parameters
    ----------
    df : pd.DataFrame
        å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
    target : str
        ç›®çš„å¤‰æ•°åˆ—å

    Returns
    -------
    str
        `"regression"` ã¾ãŸã¯ `"classification"`
    """
    return "regression" if pd.api.types.is_numeric_dtype(df[target]) else "classification"


def preprocess(
    df: pd.DataFrame, target: str, task: str
) -> Tuple[pd.DataFrame, np.ndarray, Optional[LabelEncoder]]:
    """
    LightGBM ç”¨ã®å‰å‡¦ç†ã‚’è¡Œã†ã€‚

    - ç›®çš„å¤‰æ•°æ¬ æè¡Œã‚’å‰Šé™¤
    - `object` åˆ—ã‚’ `category` ã«å¤‰æ›
    - åˆ†é¡ã‚¿ã‚¹ã‚¯æ™‚ã¯ `LabelEncoder` ã§æ•°å€¤åŒ–

    Parameters
    ----------
    df : pd.DataFrame
        å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
    target : str
        ç›®çš„å¤‰æ•°åˆ—
    task : str
        `"regression"` or `"classification"`

    Returns
    -------
    X : pd.DataFrame
        ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
    y : np.ndarray
        ç›®çš„å¤‰æ•°é…åˆ—
    le : LabelEncoder or None
        åˆ†é¡ã‚¿ã‚¹ã‚¯æ™‚ã¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã€å›å¸°ãªã‚‰ None
    """
    df = df.dropna(subset=[target]).copy()
    X = df.drop(columns=[target])
    y = df[target].values

    le: Optional[LabelEncoder] = None
    if task == "classification":
        le = LabelEncoder()
        y = le.fit_transform(y)

    # object â†’ category å¤‰æ›
    for c in X.columns:
        if X[c].dtype == "object" or pd.api.types.is_categorical_dtype(X[c]):
            X[c] = X[c].astype("category")
    return X, y, le


def align_categories(train: pd.DataFrame, test: pd.DataFrame) -> None:
    """
    ã‚«ãƒ†ã‚´ãƒªåˆ—ã®ã‚«ãƒ†ã‚´ãƒªé›†åˆã‚’ train / test ã§ä¸€è‡´ã•ã›ã€
    æœªçŸ¥ã‚«ãƒ†ã‚´ãƒªã«ã‚ˆã‚‹æ¨è«–ã‚¨ãƒ©ãƒ¼ã‚’é˜²æ­¢ã™ã‚‹ã€‚

    Parameters
    ----------
    train, test : pd.DataFrame
        å­¦ç¿’ãƒ»è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    for c in train.select_dtypes(include=["category"]).columns:
        cats = train[c].cat.categories.union(test[c].cat.categories)
        train[c] = train[c].cat.set_categories(cats)
        test[c] = test[c].cat.set_categories(cats)


# ==========================================================================
#                   æ™‚ç³»åˆ—ç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ==========================================================================
def create_future_target(df: pd.DataFrame, target: str, horizon: int) -> pd.DataFrame:
    """
    ç›®çš„å¤‰æ•°ã‚’ *horizon* è¡Œå…ˆã«ã‚·ãƒ•ãƒˆã—ã€æœªæ¥äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã€‚

    æœ«å°¾ `horizon` è¡Œã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒ NaN ã«ãªã‚‹ãŸã‚é™¤å¤–ã™ã‚‹ã€‚

    Parameters
    ----------
    df : pd.DataFrame
        å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
    target : str
        äºˆæ¸¬å¯¾è±¡åˆ—
    horizon : int
        ã©ã‚Œã ã‘å…ˆã‚’äºˆæ¸¬ã™ã‚‹ã‹ï¼ˆè¡Œæ•°ï¼‰

    Returns
    -------
    pd.DataFrame
        ã‚·ãƒ•ãƒˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
    """
    df = df.copy()
    df[target] = df[target].shift(-horizon)  # æœªæ¥ã¸ã‚·ãƒ•ãƒˆ
    return df.dropna(subset=[target]).reset_index(drop=True)


# ==========================================================================
#              RandomizedSearchCV & ãƒ¢ãƒ‡ãƒ«é–¢é€£
# ==========================================================================
def param_distributions(task: str) -> Dict:
    """
    RandomizedSearchCV ã§ä½¿ç”¨ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒã‚’è¿”ã™ã€‚

    Returns
    -------
    dict
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å â†’ scipy.stats åˆ†å¸ƒ or list
    """
    dist: Dict = {
        "n_estimators": randint(100, 601),       # 100ã€œ600
        "learning_rate": uniform(0.01, 0.19),    # 0.01ã€œ0.2
        "num_leaves": randint(15, 129),          # 15ã€œ128
        "max_depth": randint(4, 13),             # 4ã€œ12
    }
    dist["objective"] = (
        ["regression", "regression_l1"]
        if task == "regression"
        else ["binary", "multiclass"]
    )
    return dist


def create_lgbm(task: str):
    """
    ã‚¿ã‚¹ã‚¯ç¨®åˆ¥ã«å¿œã˜ãŸ LightGBM ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿”ã™ã€‚
    """
    base = dict(random_state=42)
    return lgb.LGBMRegressor(**base) if task == "regression" else lgb.LGBMClassifier(**base)


@st.cache_resource(show_spinner=False, ttl=3_600)
def train_random_search(X: pd.DataFrame, y: np.ndarray, task: str):
    """
    RandomizedSearchCV ã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©æ¢ç´¢ï¼‹å­¦ç¿’ã‚’å®Ÿæ–½ã€‚

    Parameters
    ----------
    X, y : å…¥åŠ›ç‰¹å¾´é‡ãƒ»ç›®çš„å¤‰æ•°
    task : str
        'regression' or 'classification'

    Returns
    -------
    best_estimator_, best_params_
    """
    rs = RandomizedSearchCV(
        estimator=create_lgbm(task),
        param_distributions=param_distributions(task),
        n_iter=30,  # è©¦è¡Œå›æ•°
        scoring="neg_root_mean_squared_error" if task == "regression" else "f1_macro",
        cv=3,
        n_jobs=-1,
        verbose=0,
        random_state=42,
    )
    rs.fit(X, y, categorical_feature="auto")
    return rs.best_estimator_, rs.best_params_


# ==========================================================================
#                       è©•ä¾¡æŒ‡æ¨™ã¨å¯è¦–åŒ–
# ==========================================================================
def evaluate(model, X_test, y_test, task: str, le: Optional[LabelEncoder]) -> Dict:
    """
    å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€æŒ‡æ¨™ã‚’è¾æ›¸ã§è¿”ã™ã€‚
    """
    y_pred = model.predict(X_test)
    res: Dict[str, float | np.ndarray | str] = {}
    if task == "regression":
        res["RÂ²"] = r2_score(y_test, y_pred)
        res["RMSE"] = mean_squared_error(y_test, y_pred, squared=False)
    else:
        res["Accuracy"] = accuracy_score(y_test, y_pred)
        avg = "macro" if len(set(y_test)) > 2 else "binary"
        res["F1"] = f1_score(y_test, y_pred, average=avg)
        res["Confusion"] = confusion_matrix(y_test, y_pred)
        res["Report"] = classification_report(
            y_test, y_pred, target_names=le.classes_, digits=3, zero_division=0
        )
    return res


def plot_importance(model, cols):
    """
    ç‰¹å¾´é‡é‡è¦åº¦ä¸Šä½ 20 ä»¶ã‚’æ°´å¹³ãƒãƒ¼ã§å¯è¦–åŒ–ã€‚
    """
    imp = (
        pd.DataFrame({"feature": cols, "importance": model.feature_importances_})
        .sort_values("importance", ascending=False)
        .head(20)
    )
    return px.bar(imp, x="importance", y="feature", orientation="h", title="ç‰¹å¾´é‡é‡è¦åº¦ (Top 20)")


def plot_reg(y_test, y_pred):
    """
    å›å¸°ã‚¿ã‚¹ã‚¯ã®å®Ÿæ¸¬ vs äºˆæ¸¬æ•£å¸ƒå›³ã€‚3 000 ç‚¹ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦æç”»è² è·ã‚’ä½æ¸›ã€‚
    """
    idx = np.random.choice(len(y_test), min(3_000, len(y_test)), replace=False)
    fig = px.scatter(
        x=y_test[idx],
        y=y_pred[idx],
        labels={"x": "å®Ÿæ¸¬å€¤", "y": "äºˆæ¸¬å€¤"},
        title="å®Ÿæ¸¬ vs äºˆæ¸¬ (ã‚µãƒ³ãƒ—ãƒ«)",
        trendline="ols",
        trendline_color_override="red",
    )
    fig.add_shape(
        type="line",
        x0=y_test.min(),
        y0=y_test.min(),
        x1=y_test.max(),
        y1=y_test.max(),
        line=dict(dash="dash"),
    )
    return fig


def plot_cm(cm, labels):
    """
    æ··åŒè¡Œåˆ—ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§è¡¨ç¤ºã€‚
    """
    fig = px.imshow(
        cm,
        text_auto=True,
        labels=dict(x="äºˆæ¸¬ãƒ©ãƒ™ãƒ«", y="å®Ÿæ¸¬ãƒ©ãƒ™ãƒ«"),
        x=labels,
        y=labels,
        title="æ··åŒè¡Œåˆ—",
    )
    fig.update_xaxes(side="top")
    return fig


# ==========================================================================
#                   AI Explanation (stream)
# ==========================================================================
def stream_explanation(metrics: Dict, shap_top: pd.DataFrame, task: str, api_key: str):
    """
    OpenAI Chat Completion ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã—ãªãŒã‚‰ã€
    æ´»å‹•æŒ‡æ¨™ & SHAP é‡è¦åº¦ã«åŸºã¥ããƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆãƒ»è¡¨ç¤ºã™ã‚‹ã€‚
    """
    if not _OPENAI_OK:
        st.error("openai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    client = OpenAI(api_key=api_key)

    prompt = f"""
ã‚ãªãŸã¯ç†Ÿç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€
å°‚é–€ç”¨èªã‚’æœ€å°é™ã«æŠ‘ãˆã¦éã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ã«åˆ†ã‹ã‚Šã‚„ã™ãè§£èª¬ã—ã¦ãã ã•ã„ã€‚

### ãƒ¢ãƒ‡ãƒ«æ€§èƒ½
{chr(10).join(f"- {k}: {v:.4f}" for k, v in metrics.items() if isinstance(v, float))}

### SHAP é‡è¦ç‰¹å¾´é‡ (Top 5)
{chr(10).join(f"{i+1}. {r.Feature} (å¹³å‡å½±éŸ¿åº¦: {r.MeanAbsSHAP:.4f})" for i, r in shap_top.head(5).iterrows())}

1. æŒ‡æ¨™ã«åŸºã¥ãæ€§èƒ½è©•ä¾¡ï¼ˆè‰¯ã„/æ‚ªã„ ã¨æ ¹æ‹ ï¼‰
2. ç‰¹å¾´é‡ãŒçµæœã¸ã©ã†å¯„ä¸ã—ãŸã‹
3. å¾—ã‚‰ã‚ŒãŸç¤ºå”†ã¨æ¬¡ã®æ–½ç­–
"""

    def gen():
        """
        OpenAI ã‹ã‚‰ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’é€æ¬¡ yield ã™ã‚‹
        ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã€‚
        """
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚"},
                {"role": "user", "content": prompt},
            ],
            stream=True,
            temperature=0.4,
        )
        for chunk in resp:
            delta = chunk.choices[0].delta.content
            if delta:
                yield delta

    st.subheader("ğŸ¤– AI è§£èª¬ (SHAP é€£å‹•)")
    st.write_stream(gen)


# ==========================================================================
#                          Streamlit APP
# ==========================================================================
def main() -> None:
    """
    Streamlit ã‚¢ãƒ—ãƒªã®ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆã€‚
    """
    st.set_page_config(page_title="AutoML with LightGBM", layout="wide")
    st.title("ğŸ¤– AutoML (LightGBM) â€“ æœªæ¥äºˆæ¸¬ãƒ¢ãƒ¼ãƒ‰ä»˜ã")

    # -------- Sidebar -----------------------------------------------------
    with st.sidebar:
        st.header("1ï¸âƒ£ ãƒ•ã‚¡ã‚¤ãƒ«")
        file = st.file_uploader("CSV ã‚’é¸æŠ", type="csv")
        enc_opt = st.selectbox("ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°", ["è‡ªå‹•åˆ¤å®š", "utf-8", "shift_jis", "cp932"], 0)

        st.header("2ï¸âƒ£ OpenAI API ã‚­ãƒ¼ (ä»»æ„)")
        api_key = st.text_input("API ã‚­ãƒ¼", type="password")

    # -------- Session State (åˆæœŸåŒ–) --------------------------------------
    for k in ["df_proc", "model", "X_test", "y_test", "task", "le", "metrics"]:
        st.session_state.setdefault(k, None)

    # -------- Data Load & Preprocess --------------------------------------
    if file:
        df_raw = load_data(file, None if enc_opt == "è‡ªå‹•åˆ¤å®š" else enc_opt)
        if df_raw is not None:
            st.header("ğŸ§¹ å‰å‡¦ç†")
            with st.expander("ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=True):
                st.dataframe(df_raw.head())
                st.text(f"{df_raw.shape[0]} è¡Œ Ã— {df_raw.shape[1]} åˆ—")

            col1, col2 = st.columns(2)
            with col1:
                skip = st.number_input("å‰Šé™¤ã™ã‚‹å…ˆé ­è¡Œæ•°", 0, 50, 0)
            with col2:
                drops = st.multiselect("å‰Šé™¤ã™ã‚‹åˆ—", df_raw.columns)

            if st.button("å‰å‡¦ç†ã‚’é©ç”¨", type="primary"):
                dfp = df_raw.copy()
                if skip:
                    dfp = dfp.iloc[skip:].reset_index(drop=True)
                if drops:
                    dfp.drop(columns=drops, inplace=True)
                st.session_state.df_proc = dfp
                st.success("å‰å‡¦ç†å®Œäº†")

    # -------- Training ----------------------------------------------------
    if st.session_state.df_proc is not None:
        dfp = st.session_state.df_proc
        st.header("ğŸš€ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’")
        st.dataframe(dfp.head())

        target = st.selectbox("ç›®çš„å¤‰æ•°", dfp.columns)

        # ----- æ™‚ç³»åˆ—ãƒ¢ãƒ¼ãƒ‰è¨­å®š -----
        st.subheader("æ™‚ç³»åˆ—è¨­å®š (ä»»æ„)")
        ts_mode = st.checkbox("å°†æ¥ã®å€¤ã‚’äºˆæ¸¬ã™ã‚‹ (æ™‚ç³»åˆ—ãƒ¢ãƒ¼ãƒ‰)")
        horizon = 1
        if ts_mode:
            horizon = st.number_input("ä½•è¡Œå…ˆã‚’äºˆæ¸¬ï¼Ÿ", 1, 1000, 1)
            df_future = create_future_target(dfp, target, horizon)
            st.info(f"{horizon} è¡Œå…ˆã® {target} ã‚’äºˆæ¸¬å¯¾è±¡ã¨ã—ã¾ã—ãŸã€‚ (è¡Œæ•°: {len(df_future)}/{len(dfp)})")
            df_use = df_future
        else:
            df_use = dfp

        task_auto = determine_task(df_use, target)
        task = st.radio("ã‚¿ã‚¹ã‚¯ç¨®åˆ¥", ["regression", "classification"], index=0 if task_auto == "regression" else 1)

        if st.button("ğŸ“ˆ å­¦ç¿’é–‹å§‹"):
            try:
                # ---------- å‰å‡¦ç† ----------
                with st.spinner("ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†..."):
                    X, y, le = preprocess(df_use, target, task)

                    if ts_mode:
                        # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã›ãšã€å‰ 80% ã‚’å­¦ç¿’ã€å¾Œ 20% ã‚’ãƒ†ã‚¹ãƒˆ
                        split = int(len(X) * 0.8)
                        X_tr, X_te = X.iloc[:split], X.iloc[split:]
                        y_tr, y_te = y[:split], y[split:]
                    else:
                        X_tr, X_te, y_tr, y_te = train_test_split(
                            X,
                            y,
                            test_size=0.2,
                            random_state=42,
                            stratify=y if task == "classification" else None,
                        )

                    align_categories(X_tr, X_te)

                # ---------- ãƒã‚¤ãƒ‘ãƒ‘ãƒ©æ¢ç´¢ ----------
                with st.spinner("RandomizedSearch (30 è©¦è¡Œ)..."):
                    model, best_params = train_random_search(X_tr, y_tr, task)

                # ---------- è©•ä¾¡ ----------
                mets = evaluate(model, X_te, y_te, task, le)

                # çŠ¶æ…‹ä¿å­˜ï¼ˆSHAP ãªã©å¾Œå·¥ç¨‹ã§ä½¿ç”¨ï¼‰
                st.session_state.update(
                    dict(model=model, X_test=X_te, y_test=y_te, task=task, le=le, metrics=mets)
                )

                st.success("å­¦ç¿’å®Œäº†ï¼")

                # ---------- çµæœè¡¨ç¤º ----------
                st.header("ğŸ“Š çµæœ")
                lcol, rcol = st.columns(2)

                with lcol:
                    st.subheader("æ€§èƒ½æŒ‡æ¨™")
                    for k, v in mets.items():
                        if isinstance(v, float):
                            st.metric(k, f"{v:.4f}")
                    if task == "classification":
                        st.plotly_chart(plot_cm(mets["Confusion"], le.classes_), use_container_width=True)
                        with st.expander("è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ"):
                            st.text(mets["Report"])

                with rcol:
                    st.subheader("ç‰¹å¾´é‡é‡è¦åº¦")
                    st.plotly_chart(plot_importance(model, X.columns), use_container_width=True)
                    if task == "regression":
                        st.plotly_chart(plot_reg(y_te, model.predict(X_te)), use_container_width=True)

                # ---------- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ----------
                st.download_button("ãƒ¢ãƒ‡ãƒ« (.pkl)", pickle.dumps(model), "lgbm_model.pkl")
                st.download_button(
                    "ãƒã‚¤ãƒ‘ãƒ© (.json)",
                    json.dumps(best_params, ensure_ascii=False, indent=2),
                    "best_params.json",
                )

            except Exception as e:
                st.error("å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
                # å†…éƒ¨ãƒ­ã‚°ã«ã®ã¿ãƒ•ãƒ«ã‚¹ã‚¿ãƒƒã‚¯ã‚’å‡ºåŠ›
                with open("error_log.txt", "a", encoding="utf-8") as f:
                    f.write(f"{datetime.now()}: {e}\n")

    # -------- SHAP & AI Explanation --------------------------------------
    if st.session_state.model is not None:
        st.header("ğŸ” SHAP ã§è©³ç´°è§£é‡ˆ")
        if st.button("SHAP ã‚’è¨ˆç®—ã— AI è§£èª¬"):
            try:
                import shap

                # --- SHAP è¨ˆç®— ---
                with st.spinner("SHAP è¨ˆç®—ä¸­..."):
                    explainer = shap.TreeExplainer(st.session_state.model)
                    shap_vals = explainer.shap_values(st.session_state.X_test, check_additivity=False)

                st.subheader("SHAP Summary Plot")
                shap.summary_plot(shap_vals, st.session_state.X_test, show=False)
                st.pyplot(autoresize=True, clear_figure=True)

                # SHAP å€¤ã®å¹³å‡çµ¶å¯¾å€¤ (å¤šã‚¯ãƒ©ã‚¹ã¯ã‚¯ãƒ©ã‚¹å¹³å‡)
                shap_abs = (
                    np.mean([np.abs(v) for v in shap_vals], axis=0)
                    if isinstance(shap_vals, list)
                    else np.abs(shap_vals).mean(axis=0)
                )
                shap_df = (
                    pd.DataFrame(
                        {"Feature": st.session_state.X_test.columns, "MeanAbsSHAP": shap_abs}
                    )
                    .sort_values("MeanAbsSHAP", ascending=False)
                    .reset_index(drop=True)
                )

                # --- AI è§£èª¬ ---
                if api_key:
                    with st.spinner("AI è§£èª¬ç”Ÿæˆä¸­..."):
                        stream_explanation(st.session_state.metrics, shap_df, st.session_state.task, api_key)
                else:
                    st.info("OpenAI API ã‚­ãƒ¼æœªå…¥åŠ›ã®ãŸã‚ AI è§£èª¬ã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã€‚")

            except ImportError:
                st.error("`shap` ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install shap` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            except Exception as e:
                st.error(f"SHAP/è§£èª¬ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


# ------------------ ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ ------------------
if __name__ == "__main__":
    main()
